%  -*- reftex-include-file-commands: import; -*-
% you should only have one "documentclass" line.  the following lines
% are samples that give various options.  the nofrontmatter option is
% nice because it suppresses the title and signature pages when you want
% to focus only on the main body of the thesis
%
% Friday April 10 2010 Ray Hylock <ray-hylock@uiowa.edu>
% documentclass options:
%   abstractpage            if you want to add an internal abstract (optional)
%   ackpage                 if you would like to add an acknowledgements page (optional)
%   algorithms              if you want a list of algorithms (optional)
%   appendix                if you have an appendix (optional)
%   copyrightpage           if you wish to copyright your thesis (optional)
%   dedicationpage          if you wish to make a dedication (optional)
%   epigraphpage            if you would like to add an epigraph to the beginning of your thesis (optional)
%   examples                if you want a list of examples (this uses the ntheorem package)
%   exampleslemmas          if you want a combined list of examples and lemmas (this uses the ntheorem package) (optional)
%   examplestheorems        if you want a combined list of examples and theorems (this uses the ntheorem package) (optional)
%   exampleslemmastheorems  if you want a combined list of examples, lemmas, and theorems (this uses the ntheorem package) (optional)
%   figures                 if you have any figures (this is required if you have even one figure)
%   lemmas                  if you want a list of lemmas (this uses the ntheorem package) (optional)
%   lemmastheorems          if you want a combined list of lemmas and theorems (this uses the ntheorem package) (optional)
%   nofrontmatter           suppresses the title and signiture pages for working on the body
%   tables                  if you have any tables (this is required if you have even one table)
%   theorems                if you want a list of theorems (this uses the ntheorem package) (optional)
%   phd                     if phd student; this will add the doctoral abstract (mandatory for PhD and DMA thesis candidates only)
%

% full options
%\documentclass[phd,abstractpage,copyrightpage,dedicationpage,epigraphpage,ackpage,figures,tables,lemmas,appendix]{uithesis}

% common options
%\documentclass[phd,dedicationpage,ackpage,figures,tables,appendix]{uithesis}

% example
\documentclass[phd,appendix,dedicationpage,ackpage,epigraphpage]{uithesis}

%=============================================================================
% User packages
%=============================================================================
\usepackage{bookmark}		% [recommended] for PDF bookmark generation
\usepackage{blindtext} 	% example text generation
\usepackage{mathpartir}
\usepackage{amsmath, amssymb,amsthm}
\usepackage{todonotes}
\usepackage{tikz}
\usetikzlibrary{matrix, automata,arrows, decorations.pathreplacing}
\usepackage{diagrams}
\usepackage{supertabular}
\usepackage{chronology}
\usepackage{import}
\usepackage{verbatim}
\usepackage{supertabular}
\usepackage{stmaryrd}
\usepackage{chronology}

\input{type-theory_inc}

\newcommand{\defeq}{\stackrel{\mathsf{def} }{\equiv}}
\newcommand{\Sep}[0]{\text{Sep}^3}
\newcommand{\hscase}[1]{\par\begingroup\leftskip1em #1 \par\endgroup}

%=============================================================================
% prelude
%=============================================================================

\title{The Semantic Analysis of Advanced Programming Languages}
\author{Harley Daniel Eades III}
\dept{Computer Science}

% multipleSupervisors=true for two advisors
\setboolean{multipleSupervisors}{false}
\advisor{Associate Professor Aaron Stump}
% for multiple advisors; change <value> to line up the names
%\setboolean{multipleSupervisors}{true}
%\advisor{Advisor 1\\\hspace{<value>mm}Advisor 2...}
%
% edit the names below to have your committee members names appear
% on the signature page.  memberOne should be your advisor.
%
\memberOne{Aaron Stump}
\memberTwo{Cesare Tinelli}
\memberThree{Stephanie Weirich}
\memberFour{Gregory Landini}
\memberFive{Kasturi Varadarajan}
\submitdate{June 30}
\copyrightyear{2014}

\Abstract{
\blindtext
}

\dedication{To my lovely wife, Jenny Eades.}

\epigraph{``Program testing can be used to show the presence of bugs, but never to show their absence!''
--Dijkstra (1970)}

\acknowledgements{Acknowledgements here (optional)}

\newcommand{\LBMMT}[0]{\bar{\lambda}\mu\tilde\mu}

\begin{document}

\frontmatter

\section{Introduction}
\label{sec:introduction}

There are two major problems growing in two areas.  The first is in
Computer Science, in particular software engineering. Software is
becoming more and more complex, and hence more susceptible to software
defects.  Software bugs have two critical repercussions: they cost
companies lots of money and time to fix, and they have the potential
to cause harm. 

The National Institute of Standards and Technology estimated that
software errors cost the United State's economy approximately sixty
billion dollars annually, while the Federal Bureau of Investigations
estimated in a 2005 report that software bugs cost U.S. companies
approximately sixty-seven billion a year \cite{nist02,fbi05}.

Software bugs have the potential to cause harm.  In 2010 there were a
approximately a hundred reports made to the National Highway Traffic
Safety Administration of potential problems with the braking system of
the 2010 Toyota Prius \cite{Consumer:2010}.  The problem was that the
anti-lock braking system would experience a ``short delay'' when
the brakes where pressed by the driver of the vehicle
\cite{thedetroitbureau.com:2009}.  This actually caused some crashes.
Toyota found that this short delay was the result of a software bug,
and was able to repair the the vehicles using a software update
\cite{Reuters:2009}.  Another incident where substantial harm was
caused was in 2002 where two planes collided over \"{U}berlingen in
Germany. A cargo plane operated by DHL collided with a passenger
flight holding fifty-one passengers.  Air-traffic control did not
notice the intersecting traffic until less than a minute before the
collision occurred.  Furthermore, the on-board collision detection
system did not alert the pilots until seconds before the collision.
It was officially ruled by the German Federal Bureau of Aircraft
Accidents Investigation that the on-board collision detection was
indeed faulty \cite{Collision:2004}.

The second major problem affects all of science.  Scientific
publications are riddled with errors.  A portion of these errors are
mathematical.  In 2012 Casey Klein et al. used specialized computer
software to verify the correctness of nine papers published in the
proceedings of the International Conference on Functional Programming
(ICFP).  Two of the papers where used as a control which where known
to have been formally verified before.  In their paper
\cite{Klein:2012} they show that all nine papers contained
mathematical errors.  This is disconcerting especially since most
researchers trust published work and base their own work off of these
papers.  Kline's work shows that trusting published work might result
in wasted time for the researchers basing their work off of these
error prone publications.  Faulty research hinders scientific
progress.

Both problems outlined above have been the focus of a large body of
research over the course of the last forty years.  These challenges
have yet to be completed successfully.  The work we present here makes
up the foundations of one side of the programs leading the initiative
to build theory and tools which can be used to verify the correctness
of software and mathematics.  This program is called program
verification using dependent type theories.  The second program is
automated theorem proving.  In this program researchers build tools
called model checkers and satisfiability modulo-theories solvers.
These tools can be used to model and prove properties of large complex
systems carrying out proofs of the satisfiability of certain
constraints on the system nearly automatically, and in some cases
fully automatically.  As an example Andr\'{e} Platzer and Edmund
Clarke in 2009 used automated theorem proving to verify the
correctness of the in flight collision detection systems used in
airplanes.  They actually found that there were cases where two plans
could collide, and gave a way to fix the problem resulting in a fully
verified algorithm for collision detection.  That is he mathematically
proved that there is no possible way for two plans to collide if the
systems are operational \cite{DBLP:conf/fm/PlatzerC09}.  Automated
theorem provers, however, are tools used to verify the correctness of
software externally to the programming language and compiler one uses
to write the software.  In contrast with verification using dependent
types we wish to include the ability to verify software within the
programming language being used to write the software. Both programs
have their merits and are very fruitful and interesting.

Every formal language within this thesis has been formally defined in
a tool called Ott \cite{Sewell:2010}.  In addition, the full Ott
specification of every type theory defined with in this thesis can be
found in the appendix.  Ott is a tool for writing definitions of
logics, programming languages, type theories, $\lambda$-calculi, and
any other formal language that consists of syntax and inference-style
rules.  Ott generates a parser and a type checker which is used to
check the accuracy of all objects definable with in the language given
to Ott as input.  Ott's strongest application is to check for syntax
errors within research articles.  Ott is a great example of a tool
using the very theory we are presenting in this article.  It clearly
stands as a successful step towards the solution of the second major
problem outlined above.
% section introduction (end)

\section{Novel Contributions}
\label{sec:novel_contributions}
This thesis consists of two major topics.  The first topic is on the
design of general purpose dependently-typed functional programming
languages. This topic is covered in Part~\ref{part:design} (Design).
The second topic is on the analysis of dependently-typed functional
programming languages and various type theories.  This topic is broken
up into two parts: Part~\ref{part:basic-analysis} (Basic Syntactic
Analysis) and Part~\ref{part:norm-hs} (Normalization by Hereditary
Substitution).  It is the content of these parts that consists of
novel research contributions. Specifically, the following list briefly
outlines each contribution:
\begin{itemize}
\item The design and analysis of a core dependently-typed functional
  programming language with a new property called freedom of
  speech. See Chapter~\ref{chap:freedom_of_speech} and
  Chapter~\ref{chap:freedom_of_speech_anal}.

\item The full design of a core dependently-typed functional
  programming language called Separation of Proof from Program ($\Sep$) that
  remedies several short comings of the freedom of speech language.
  This is a full programming language that has been implemented, but
  this implementation is not a contribution of this thesis.  See
  Chapter~\ref{chap:separation_of_proof_from_program}.

\item The design and analysis of a new logic and corresponding type
  theory called Dualized Intuitionistic Logic (DIL) and Dualized Type
  Theory (DTT) respectively. We introduce a new completely symmetric
  syntax that makes for a beautiful definition of the two theories.
  See Chapter~\ref{chap:dualized_type_theory_de} and
  Chapter~\ref{chap:dualized_type_theory_anal}.

\item We prove weak normalization of an entire family of predicative
  type theories based on Stratified System F using a proof technique
  called hereditary substitution. See
  Chapter~\ref{chap:stratified_system_f_and_beyond}.

\item Similarly, we show that hereditary substitution can be extended
  to prove normalization of a classical type theory called the
  $\lambda\Delta$-calculus. See
  Chapter~\ref{chap:the_lambdadelta-calculus}.

\item The final contribution is the brief history of type theory given
  in Part~\ref{part:background}.  There we try and highlight the
  significant contributions of type theory starting with Russell.
  This history is by no means complete, but we provide the complete
  definition of many significant type theories.  This tries to provide
  a one stop shop for an introduction to the field.
\end{itemize}
% section novel_contributions (end)

\part{Background}          
\label{part:background}
\import*{background/}{background}
% part background (end)

\part{Design}
\label{part:design}
The design of any programming language must facilitate reasoning about
the programming language itself.  This facilitation comes in the form
of a rigorous definition which makes mathematically precise all of the
structure of the programming language.  This provides a means for
researchers and implementors to fully understand the limits of the
language.  Contrary to what programming language designers have done
in the past this rigors definition does not only include the syntax,
but also includes mathematical definitions of the type system as well
as an interpreter. This is a significant benefit of basing programming
languages on type theories.  As we have seen a type theory must be
rigorously defined.  

Considering the motivation we gave in the introduction, a mathematical
rigorous design provides the necessary structure to allow programs to
mathematical reason about the programs they write in the programming
language.  In addition, this makes it possible for designers to reason
about the correctness of the language itself, and thus allow them to
make strong guarantees of the correctness of the language to their
programmers.  To prevent major bugs programming languages must be
rigorously defined.  If we do not know what the programming language
allows, then we cannot be sure what is safe.

Recall from Chapter~\ref{chap:the_three_perspectives} that the
computational trinity states that type theories are simultaneously a
logic as well as a programming language, and this double perspective
provides the means of verifying properties of the programs we write in
the theory, but this feature comes with a strong invariant, every
program must terminate.  Now suppose we are working on a large
development in a programming language based on type theory like
Martin-L\"of's Type Theory or CCC which has come to a difficult
problem that can be solved by the definition of some terminating
function.  Furthermore, suppose we have found the solution in some
research paper, but the termination proof is very complex and uses an
advanced semantic proof technique, and lastly suppose that this
termination proof is not formalized in a type theory, but is an
informal proof that we trust.  Since we are conducting our development
in a terminating type theory there is only one course of action.  We
have to formalize this complex termination argument. This can be
devastating to the development, because doing such a proof may take
months, even years to complete!  Another hypothetical is supposing we
do not care if a program terminates.  There are correctness properties
that hold regardless of termination.  For example, consider
associativity of list append -- using a pseudo syntax:
\[ \forall (A\,B\,C :
\mathsf{List}\,\mathsf{Nat}).\mathsf{append}\,A\,(\mathsf{append}\,B\,C)
= \mathsf{append}\,(\mathsf{append}\,A\,B)\,C \] This equation holds
regardless of termination of $A$, $B$, or $C$, because if anyone of
them happen to diverge, then both sides of the equation diverge.  

So the natural question is can we design a programming language based
on type theory that allows for non-terminating programs, but also be
able to verify properties of the programs we write in the language?
It turns out that we can, and in the next two chapters we introduce
the designs of two new dependently-typed functional programming
languages called Freedom of Speech and Separation of Proof from
Program that amount to solutions to this question.

Directly supporting non-termination is one solution, but there is
another possibility.  Non-terminating functions can be simulated using
the notion of coinduction which is dual to induction.  Coinduction
provides a means to define and observe infinite streams of data.
Furthermore, coinductive types push the notion of non-termination into
the types. There are a large number of applications of coinduction,
and even more when we allow the mixture of induction and coinduction,
for example the definition of an infinite stream of trees requires
their mixture.  However, currently there is no known type theory that
supports both induction and coinduction, and supports their
(unrestricted) mixture while maintaining type safety.  In the third
chapter of Design we introduce a new logic and corresponding type
theory called Dualized Intuitionistic Logic and Dualized Type Theory
respectively that is based on a logic rich in duality that shows
promise of being a logical framework for induction and coinduction
that is type safe and supports their mixture.

\chapter{Freedom of Speech}
\label{chap:freedom_of_speech}
\import*{free-speech/}{FS-design-inc}
% chapter freedom_of_speech (end)

\chapter{Separation of Proof from Program}
\label{chap:separation_of_proof_from_program}
\import*{sep3/}{sep3-inc}
% chapter separation_of_proof_from_program (end)

\chapter{Dualized Logic and Type Theory}
\label{chap:dualized_type_theory_de}
\import*{dil-dtt/}{design-inc}
% chapter dualized_type_theory (end)
% part design (end)

\part{Basic Syntactic Analysis}
\label{part:basic-analysis}
The following chapters conduct basic meta-theoretic analysis of the
freedom of speech language, and dualized logic and its corresponding
type theory dualized type theory.  We consider these analysis to be
basic because they are straightforward applications of existing
techniques, and consist of the most basic properties that should hold
for any programming language.

At least a basic meta-theoretic analysis of a programming language is
very important.  Every programming language should at least have been
proven type safe, and if the language contains a logical fragment,
then it should be proven consistent.  These give strong guarantees to
the programmer.  Type safety ensures that if a program has a type,
then it can either compute something, or is the answer itself.
Furthermore, it ensures that if a program has a type, then after
running it, the result has the same type.  Lastly, consistency -- as
we have mentioned before -- ensures that the objects of the language
we call proofs really are proof.  These sound like commonsense
properties, and they are, but many programming language implementors
fail to establish them.  A programming language without such
guarantees would amount to a large number of significant bugs;
e.g. the ones we mentioned in the introduction.  Therefore, a basic
meta-theoretic analysis is necessary to be able to trust the programs
we write, and the verification of programs carried out within the
languages themselves.

\chapter{Freedom of Speech}
\label{chap:freedom_of_speech_anal}
\import*{free-speech/}{FS-analysis-inc}
% chapter freedom_of_speech (end)

\chapter{Dualized Logic and Type Theory}
\label{chap:dualized_type_theory_anal}
\import*{dil-dtt/}{analysis-inc}
% chapter dtt (end)
% part basic-analysis (end)

\part{Normalization by Hereditary Substitution}
\label{part:norm-hs}

One motivation for the work in this thesis is that programming
languages must contain the means of verifying the software written in
them.  This verification would catch major bugs during development as
opposed to after the software is released to the public.  For example,
the bug found in the breaking system of the 2010 Prius would have
never been released, and thus no accidents would have occurred.  A
programming language that contains the ability to verify programs
would have to contain some notion of a logic, and this logic must be
trusted.  That is, the proofs written in this logic must be true; this
corresponds to logical consistency.

We have seen several ways of proving consistency both in the
background section of this thesis and in the previous part on the
basic syntactic analysis of the freedom of speech language, as well as
DTT.  The usual method of proving consistency is through proving weak
normalization (or simply normalization) of the programming language or
type theory.  Often Tait's reducibility or Girard's reducibility
candidates are the proof techniques used for showing both weak and
strong normalization, but these techniques are very complex, and
require advanced logical features to be able to formalize arguments
using such techniques in a proof assistant like Coq or Agda.  Thus, it
is worthwhile to investigate other proof techniques for proving
normalization that show promise of being less complex.  The hereditary
substitution proof technique fits this goal perfectly.  For example,
in general, large eliminations are not needed when formalizing a
hereditary substitution based proof of normalization, but this is not
the case for a proof using reducibility.

There is one caveat when it comes to the hereditary substitution proof
technique, it is not known which type theories it can be applied to.
This is the contribution of each of the following chapters.  We first
give several proofs of normalization using hereditary substitution for
several extensions of stratified system F (SSF), and then give the
first proof of normalization by hereditary substitution for a
classical type theory called the $\lambda\Delta$-calculus.  Throughout
all of the following chapters we assume the reader is familiar with
hereditary substitution.  If the reader is not, then they should first
read Section~\ref{sec:hereditary_substitution}.

\chapter{Stratified System F and Beyond}
\label{chap:stratified_system_f_and_beyond}
\import*{norm-by-hs/ssf/}{ssf.tex}
\import*{norm-by-hs/ssf+/}{ssf+.tex}
\import*{norm-by-hs/dep-ssf/}{dep-ssf.tex}
\import*{norm-by-hs/ssf-omega/}{ssf-omega.tex}
% chapter stratified_system_f_and_beyond (end)

\chapter{The $\lambda\Delta$-Calculus}
\label{chap:the_lambdadelta-calculus}
\import*{norm-by-hs/lam-delta/}{lam-delta-inc.tex}
% chapter the_lambdadelta-calculus (end)

% part norm-hs (end)

\chapter{Conclusion}
\label{chap:conclusion}

Bugs are the bain of software development, and in order to rid the
world of such problems we must revisit the very foundations of
programming languages.  We have argued that a programming language
must be mathematically defined.  This allows programmers and language
designers the ability to reason about the software being developed in
the language.  Furthermore, we have argued that the programming
languages of the future must contain some notion of a logic which
allows for the verification of the programs written in the languages.

We introduced two new dependently-typed functional programming
languages that contain a logical fragment called Freedom of Speech and
Separation of Proof from Program.  We proved logical consistency of
logical fragment of the former.  Furthermore, we introduced a new type
theory called Dualized Type Theory that shows promise of being a
logical foundation of induction and co-induction.  All of these new
theories constitue foundations of programming languages that directly
support verification of the software developed in them.

Finally, we showed how to adapt the hereditary substitution proof
technique for showing logical consistency by establishing
normalization of several extensions of Stratified System F.  We also
proved normalization by hereditary substitution of the
$\lambda\Delta$-calculus; the first classical type theory to be proved
consistent using hereditary substitution.  These proofs show that
hereditary substitution is a valuable tool that can be used to
establish consistency of the logical fragment of various programming
languages.

We believe the future is bright, and that investigations such as the
ones in this thesis will lead to new and exciting programming
languages.  These new languages will have the means to prevent major
bugs in safety critical devices from reaching the public, but there is
a lot more work to be done.

\section{Future Work}
\label{sec:future_work}
The following list addresses future work related to each theory:
\begin{itemize}
\item[-] The freedom of speech language has a call-by-value
  operational semantics, but this requires many unfortunate value
  restrictions.  It would be worthwhile to explore the language design
  when call-by-name is adopted.  Some questions to ask are is the
  language more elegant?  That is, do we trade the value restrictions
  for some other restrictions?

\item[-] We only presented the design of the $\Sep$, but the analysis
  still needs to be done.  In particular, consistency should be
  established for the proof fragment, and type safety should be proven
  for the entire language.

\item[-] DTT is a nice start at understanding if bi-intuitionistic
  logic can be the basis of a logical framework for induction and
  co-induction, but it is not clear that DTT can be conservatively
  extended with such features and maintain type safety and
  consistency.  An exploration of these features is an important
  future goal.  Furthermore, the language design containing
  abstrctions from the semantics (abstract Kripke graphs, and worlds
  on types) is an unfortunate consequence of the failure of
  cut-elimination due to subtraction, are their alternate designs that
  could be used which prevent the need for these abstractions?

\item[-] Hereditary substitution still needs to be explored with
  respect to more expressive type theories.  For example, can it be
  used to prove normalization for a type theory with large
  eliminations?  How about impredicative theories like system F?  My
  conjecture is that it can, but it will require new ideas, because
  the orderings we have used to prove correctness of the hereditary
  substitution function are too weak.
\end{itemize}

% section future_work (end)

% section conclusion (end)


%=============================================================================
\appendix
%=============================================================================

\chapter{Type Theories}
\label{chap:type_theories}
\section{The $\lambda$-Calculus}
\label{sec:the_lambda-calculus}
\Lamall{}
% section the_lambda-calculus (end)

\newpage
\section{Church-Style Simply Typed $\lambda$-Calculus}
\label{sec:church_style_simply_typed_lambda-calculus}
\CHSTLCall{}
% section church_style_simply_typed_lambda-calculus (end)

\newpage
\section{Curry-Style Simply Typed $\lambda$-Calculus}
\label{sec:curry_style_simply_typed_lambda-calculus}
\CSTLCall{}
% section curry_style_simply_typed_lambda-calculus (end)

\newpage
\section{Combinatory Logic}
\label{sec:combinatory_logic}
\Comball{}
% section combinatory_logic (end)

\newpage
\section{G\"odel's System T}
\label{sec:godels_system_t}
\Tall{}
% section godels_system_t (end)

\newpage
\section{Girard/Reynold's System F}
\label{sec:girard-reynolds_system_f}
\Fall{}
% section girard-reynolds_system_f (end)

\newpage
\section{Stratified System F}
\label{sec:stratified_system_f}
\SSFall{}
% section girard-reynolds_system_f (end)

\newpage
\section{System $\text{F}^\omega$}
\label{sec:system_fw}
\Fwall{}
% section system_fw (end)

\newpage
\section{The $\lambda\mu$-Calculus}
\label{sec:lamu_all}
\Lamuall{}
% section lamu_all (end)

\newpage
\section{The $\lambda\Delta$-Calculus}
\label{sec:lamd_all}
\Lamdall{}
% section lamd_all (end)

\newpage
\section{The $\LBMMT$-Calculus}
\label{sec:lbmmt_all}
\LBMMTall{}
% section lbmmt_all (end)

\newpage
\section{The Dual-Calculus}
\label{sec:dc_all}
\DCall{}
% section dual_calculus_all (end)

\newpage
\section{Martin-L\"of's Type Theory}
\label{sec:tt_all}
\TTall{}
% section tt_all (end)

\newpage
\section{The Calculus of Constructions}
\label{sec:coc_all}
\CoCall{}
% section coc_all (end)

\newpage
\section{The Separated Calculus of Constructions}
\label{sec:coc_sep_all}
\CoCSall{}
% section coc_sep_all (end)

\newpage
\section{Freedom of Speech}
\label{sec:freedom_of_speech_all}
\FSall{}
% section freedom_of_speech_all (end)

\newpage
\section{Annotated Separation of Proof from Program}
\label{sec:annotated_separation_of_proof_from_program}
\Sepall{}
% section annotated_separated_of_proof_from_program (end)

\newpage
\section{Unannotated Separation of Proof from Program}
\label{sec:unannotated_separation_of_proof_from_program}
\SepUall{}

\noindent
The following defines the eraser functions.
\begin{definition}
  \label{def:eraser_function}
  The erasers are defined as follows:
  \begin{itemize}
  \item Super Kinds:\\
    \begin{math}
      \begin{array}{lll}
        |\mathsf{LogicalKind}_{[[i]]}| & = & \mathsf{LogicalKind}_{[[i]]}
      \end{array}
    \end{math}
  \item Logical Kinds:\\
    \begin{math}
      \begin{array}{lll}
        |[[x]]|                   & = & x\\
        |\mathsf{Formula}_{[[i]]}| & = & \mathsf{Formula}_{{[[i]]}}\\
        |[[Forall x : A . LK]]|   & = & \forall [[x]] : |[[A]]|.|[[LK]]|
      \end{array}
    \end{math}
    
  \item Predicates:\\
    \begin{math}
      \begin{array}{lll}
        |[[\ L x : A . P]]|       & = & \Lambda [[x]] . |[[P]]|\\
        |[[P a]]|                 & = & |P|\ |a|\\
        |[[Forall x : A . P]]|    & = & \forall [[x]]:|[[A]]|.|[[P]]|\\
        |[[t1 = t2]]|             & = & |[[t1]]| = |[[t2]]|\\
        |[[t !]]|                 & = & |[[t]]|!\\
        |[[let x = p in P]]|      & = & \mathsf{let}\,[[x]] = |[[p]]|\,in\,|[[P]]|\\
        |[[let x = P in P']]|     & = & \mathsf{let}\,[[x]] = |[[P]]|\,in\,|[[P']]|\\
        |[[let x = t [x'] in P]]| & = & \mathsf{let}\,[[x]] = |[[t]]|\,in\,|[[P]]|\\
        |[[P1 + P2]]|             & = & |[[P1]]| + |[[P2]]|\\
        |[[Exists x : A . P]]|    & = & \exists [[x]]:|[[A]]|.|[[P]]|\\
        |[[bot i]]|               & = & [[bot i]]\\
        |[[t < t']]|              & = & |[[t]]| < |[[t']]|
      \end{array}
    \end{math}
    
  \item Proofs:\\
    \begin{math}
      \begin{array}{lll}
        |[[x]]| & = & x\\
        |[[\ L x : A . p]]| & = & \Lambda [[x]].|[[p]]|\\
        |[[p p']]| & = & |[[p]]|\,|[[p']]|\\
        |[[p t]]| & = & |[[p]]|\,|[[t]]|\\
        |[[injl p with P]]| & = & \mathsf{injl}\,|[[p]]|\\
        |[[injr p with P]]| & = & \mathsf{injr}\,|[[p]]|\\
        |[[case p of x . p' , y . p'']]| & = & \mathsf{case}\,|[[p]]|\,\mathsf{of}\,[[x]] . |[[p']]| , [[y]] . |[[p'']]|\\
        |[[(a , p ) as P]]| & = & (|[[a]]|,|[[p]]|)\\
        |[[case p1 of ( x , y ) . p2]]| & = & \mathsf{case}\,|[[p1]]|\,\mathsf{of}\,( [[x]] , [[y]] ) . |[[p2]]|\\
        |[[let x = p' in p]]| & = & \mathsf{let}\,[[x]] = |[[p']]|\,in\,|[[p]]|\\
        |[[let x = P in p]]| & = & \mathsf{let}\,[[x]] = |[[P]]|\,in\,|[[p]]|\\
        |[[let x = t [x'] in p]]| & = & \mathsf{let}\,[[x]] = |[[t]]|\,in\,|[[p]]|\\
        |[[join t1 t2]]| & = & \mathsf{join}\\
        |[[conv p by q1 ... qn at x1 ... xn . P]]| & = & (\lambda x_1.\cdots.\lambda x_m.|[[p]]|)\,|q_1|\,\cdots\,|q_m|\\
        |[[predconv p P]]| & = & |[[p]]|\\
        |[[valax t]]| & = & \mathsf{valax}\\
        |[[case t [ x ] p of R]]| & = & \mathsf{case}\,|[[t]]|\,\mathsf{of}\,|[[R]]|\\
        |[[tcase t [ x ] of abort -> p1 | ! -> p2]]| & = & 
        \mathsf{tcase}\,|[[t]]|\,\mathsf{of}\,[[abort]] \to |[[p1]]|\,|\,! \to |[[p2]]|\\
        |[[ind f x : t , p1 . p2]]| & = & \mathsf{ind}\,f\,[[x]].|[[p2]]|\\
        |[[contra p1]]| & = & \mathsf{contra}\,|[[p1]]|\\
        |[[contraval p1 p2]]| & = & \mathsf{contraval}\,|[[p1]]|\,|[[p2]]|\\
      \end{array}
    \end{math}
    
  \item Proof branches ($[[R]]$):\\
    \begin{math}
      \begin{array}{lll}
        |[[Ctor C x1 ep1 dots xn epn => p | R]]| & = & |[[Ctor C x1 ep1 dots xn epn]]| \Rightarrow |[[p]]|\,|\,|[[R]]|\\
        |[[done]]| & = & done 
      \end{array}
    \end{math}

  \item Terms: \\
    \begin{math}
      \begin{array}{lll}
        |[[x]]| & = & x\\
        |\mathsf{Type}_{[[i]]}| & = & \mathsf{Type}_{[[i]]}\\
        |[[Pi + x : A . t]]| & = & \Pi [[x]]_{+} : |[[A]]|.|[[t]]|\\
        |[[Pi - x : A . t]]| & = & \Pi [[x]]_{-} : |[[A]]|.|[[t]]|\\
        |[[\ P + x : LK . t]]| & = & |[[t]]|\\
        |[[\ P - x : LK . t]]| & = & |[[t]]|\\
        |[[\ P + x : P . t]]| & = & |[[t]]|\\
        |[[\ P - x : P . t]]| & = & |[[t]]|\\
        |[[\ P + x : t' . t]]| & = & \lambda [[x]].|[[t]]|\\
        |[[\ P - x : t' . t]]| & = & |[[t]]|\\
        |[[conv t by q1 ... qn at x1 ... xm . t']]| & = & |[[t']]|\\
        |[[case t [ x ] of H]]| & = & \mathsf{case}\,|[[t]]|\,\mathsf{of}\,|[[H]]|\\
        |[[t t' +]]| & = & |[[t]]|\,|[[t']]|\\
        |[[t t' -]]| & = & |[[t]]|\\
        |[[t p +]]| & = & |[[t]]|\\
        |[[t p -]]| & = & |[[t]]|\\
        |[[t P +]]| & = & |[[t]]|\\
        |[[t P -]]| & = & |[[t]]|\\
        |[[let x = p in t]]|      & = & |[[ [p/x]t]]|\\
        |[[let x = P in t']]|     & = & |[[ [P/x]t']]|\\
        |[[let x = t [y] in t']]| & = & \mathsf{let}\,[[x]] = |[[t]]|\,in\,|[[t']]|\\
        |[[tcast t by p]]| & = & \mathsf{tcast}\,|[[t]]|\\
        |[[abort t]]| & = & \mathsf{abort}\\
        |[[rec f x : t1 . t2]]| & = & \mathsf{rec}\,[[f]]\,[[x]].|[[t2]]|\\
        |[[Ctor C ]]| & = & \Sepkw{C}\\
      \end{array}
    \end{math}
    
  \item Term branches ($[[H]]$):\\
    \begin{math}
      \begin{array}{lll}
        |[[Ctor C x1 ep1 dots xn epn => t | H]]| & = & |[[Ctor C x1 ep1 dots xn epn]]| \Rightarrow |[[t]]|\,|\,|[[H]]|\\
        |[[done]]| & = & done 
      \end{array}
    \end{math}

  \item Logical Contexts ($[[G]]$):\\
    \begin{math}
      \begin{array}{lll}
        |\cdot| = \cdot\\
        |[[G,x : g A]]| = |[[G]]|,[[x]]:^[[g]] |[[A]]|
      \end{array}
    \end{math}

  \item Programmatic Contexts ($[[G]]$):\\
    \begin{math}
      \begin{array}{lll}
        |\cdot| = \cdot\\
        |[[G, x : g t]]| = |[[G]]|,[[x]]:^[[g]] |[[t]]|\\
        |[[G, x : g P]]| = |[[G]]|\\
        |[[G, x : g LK]]| = |[[G]]|\\
      \end{array}
    \end{math}
    
  \item Constructor Sets ($[[l]]$):\\
    \begin{math}
      \begin{array}{lll}
        |[[empty]]|                 = \emptyset\\
        |\{ [[Ctor C]] : [[t']]\}|  = \{ [[Ctor C]]:|[[t]]| \}\\
        |[[l1]] \cap |[[l2]]|       = |[[l1]]| \cap |[[l2]]|\\
        |[[l1]] - [[l2]]|           = |[[l1]]| - |[[l2]]|\\
        |[[l1]] = [[l2]]|           = |[[l1]]| = |[[l2]]|\\\\
      \end{array}
    \end{math}

  \item Sets of Datatype Constructors ($[[M]]$):\\
    \begin{math}
      \begin{array}{lll}
        |\mathsf{nil}| = \mathsf{nil}\\
        |[[(Ctor C : t') :: M]]| = ([[Ctor C]] : |[[t']]|)::|[[M]]| \\
      \end{array}
    \end{math}
    
  \item Logical Signatures ($[[D]]$):\\
    \begin{math}
      \begin{array}{lll}
        |\cdot| = \cdot\\
        |[[D , ( C , t , M )]]| = |[[D]]|, ([[C]], |[[t]]|, |[[M]]|)\\
        |[[D , x = ( a , A )]]| = |[[D]]| , [[x]] = ( |[[a]]| , |[[A]]| )
      \end{array}
    \end{math}

  \item Programmatic Signatures ($[[D]]$):\\
    \begin{math}
      \begin{array}{lll}
        |\cdot| = \cdot\\
        |[[D , ( C , t , M )]]| = |[[D]]|, ([[C]], |[[t]]|, |[[M]]|)\\
        |[[D , x = ( P , LK )]]| = |[[D]]|\\
        |[[D , x = ( p , P )]]| = |[[D]]|\\
        |[[D , x = ( t , t' )]]| = |[[D]]| , [[x]] = ( |[[t]]| , |[[t']]| )
      \end{array}
    \end{math}
  \end{itemize}
\end{definition}
% section annotated_separation_of_proof_from_program (end)
% section type_theories (end)

%=============================================================================
% bibliography
%=============================================================================
\interlinepenalty=10000	% prevents bib items from splitting across pages
\bibliographystyle{uithesis}
\bibliography{thesis}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
