%  -*- reftex-include-file-commands: import; -*-
% you should only have one "documentclass" line.  the following lines
% are samples that give various options.  the nofrontmatter option is
% nice because it suppresses the title and signature pages when you want
% to focus only on the main body of the thesis
%
% Friday April 10 2010 Ray Hylock <ray-hylock@uiowa.edu>
% documentclass options:
%   abstractpage            if you want to add an internal abstract (optional)
%   ackpage                 if you would like to add an acknowledgements page (optional)
%   algorithms              if you want a list of algorithms (optional)
%   appendix                if you have an appendix (optional)
%   copyrightpage           if you wish to copyright your thesis (optional)
%   dedicationpage          if you wish to make a dedication (optional)
%   epigraphpage            if you would like to add an epigraph to the beginning of your thesis (optional)
%   examples                if you want a list of examples (this uses the ntheorem package)
%   exampleslemmas          if you want a combined list of examples and lemmas (this uses the ntheorem package) (optional)
%   examplestheorems        if you want a combined list of examples and theorems (this uses the ntheorem package) (optional)
%   exampleslemmastheorems  if you want a combined list of examples, lemmas, and theorems (this uses the ntheorem package) (optional)
%   figures                 if you have any figures (this is required if you have even one figure)
%   lemmas                  if you want a list of lemmas (this uses the ntheorem package) (optional)
%   lemmastheorems          if you want a combined list of lemmas and theorems (this uses the ntheorem package) (optional)
%   nofrontmatter           suppresses the title and signiture pages for working on the body
%   tables                  if you have any tables (this is required if you have even one table)
%   theorems                if you want a list of theorems (this uses the ntheorem package) (optional)
%   phd                     if phd student; this will add the doctoral abstract (mandatory for PhD and DMA thesis candidates only)
%

% full options
%\documentclass[phd,abstractpage,copyrightpage,dedicationpage,epigraphpage,ackpage,figures,tables,lemmas,appendix]{uithesis}

% common options
%\documentclass[phd,dedicationpage,ackpage,figures,tables,appendix]{uithesis}

% example
\documentclass[phd,appendix,dedicationpage,ackpage,epigraphpage]{uithesis}

%=============================================================================
% User packages
%=============================================================================
\usepackage{bookmark}		% [recommended] for PDF bookmark generation
\usepackage{blindtext} 	% example text generation
\usepackage{mathpartir}
\usepackage{amsmath, amssymb,amsthm}
\usepackage{todonotes}
\usepackage{tikz}
\usetikzlibrary{matrix, automata,arrows, decorations.pathreplacing}
\usepackage{diagrams}
\usepackage{supertabular}
\usepackage{chronology}
\usepackage{import}
\usepackage{verbatim}
\usepackage{supertabular}
\usepackage{stmaryrd}
\usepackage{chronology}
\usepackage{makeidx}            % Enable indexing.
\input{type-theory_inc}

\newcommand{\defeq}{\stackrel{\mathsf{def} }{\equiv}}
\newcommand{\Sep}[0]{\text{Sep}^3}
\newcommand{\hscase}[1]{\par\begingroup\leftskip1em #1 \par\endgroup}

%=============================================================================
% prelude
%=============================================================================

\title{The Semantic Analysis of Advanced Programming Languages}
\author{Harley Daniel Eades III}
\dept{Computer Science}

% multipleSupervisors=true for two advisors
\setboolean{multipleSupervisors}{false}
\advisor{Associate Professor Aaron Stump}
% for multiple advisors; change <value> to line up the names
%\setboolean{multipleSupervisors}{true}
%\advisor{Advisor 1\\\hspace{<value>mm}Advisor 2...}
%
% edit the names below to have your committee members names appear
% on the signature page.  memberOne should be your advisor.
%
\memberOne{Aaron Stump}
\memberTwo{Cesare Tinelli}
\memberThree{Stephanie Weirich}
\memberFour{Gregory Landini}
\memberFive{Kasturi Varadarajan}
\submitdate{August 2014}
\copyrightyear{2014}

\Abstract{
\blindtext
}

\dedication{To my lovely wife, Jenny Eades.}

\epigraph{``Program testing can be used to show the presence of bugs, but never to show their absence!''
--Dijkstra (1970)}

\acknowledgements{The first person I would like to acknowledge is my
  advisor Aaron Stump.  He is one of the kindest and most intelligent
  people I have had the pleasure to work with, and without his
  guidance I would have never made it this far. I can only hope to
  acquire the insight and creativity you have when working on a
  research problem. Furthermore, I would like to thank him for
  introducing me to my research area in type
  theory and the foundations of functional programming languages.  \\
  \indent Secondly, I would like to thank my wife, Jenny Eades, whose
  hard work literally made it possible for there to be food on our
  table and a roof over our heads. She is a remarkable person who I
  cherish; without her I would be lost. \\ \indent I would like to
  thank my family, especially my parents, Harley and Judy Eades, for
  their support, and my brother, Steve Eades, who let Jenny and I
  sleep at his house when we visited over the course of the last five
  years.  My family always reminded me to climb out of the office, and
  that it was okay to have fun outside of research. \\ \indent Cesare
  Tinelli taught me about rigor and how to probe deep into a research
  article to get at the heart of the matter.  I would like to thank
  him for that. \\ \indent Stephanie Weirich is an amazing researcher
  with great ideas, and I learned a lot from her.  I am very thankful
  for the time I got to spend working
  with her especially during the summer of 2012. \\
  \indent I would also like to thank all of the members of the
  University of Iowa Computational Logic Center.  I have really
  enjoyed interacting with them all both in research and personally.
  I learned a tremendous amount during our semester seminars.  \\
  \indent Most of my research over the course of the last five years
  was part of the Trellys project.  I learned a lot from every member
  of the project.  So I would like to thank them all for their many
  conversations, especially during the yearly project meetings.  These
  were extremely fun, and we always had great homemade pizza.  If I
  forgot anyone, then I am sorry, and I thank you. }

\newcommand{\LBMMT}[0]{\bar{\lambda}\mu\tilde\mu}

\makeindex

\begin{document}

\frontmatter

\section{Introduction}
\label{sec:introduction}

There are two major problems growing in two areas.  The first is in
Computer Science, in particular software engineering. Software is
becoming more and more complex, and hence more susceptible to software
defects.  Software bugs have two critical repercussions: they cost
companies lots of money and time to fix, and they have the potential
to cause harm. 

The National Institute of Standards and Technology estimated that
software errors cost the United State's economy approximately sixty
billion dollars annually, while the Federal Bureau of Investigations
estimated in a 2005 report that software bugs cost U.S. companies
approximately sixty-seven billion a year \cite{nist02,fbi05}.

Software bugs have the potential to cause harm.  In 2010 there were a
approximately a hundred reports made to the National Highway Traffic
Safety Administration of potential problems with the braking system of
the 2010 Toyota Prius \cite{Consumer:2010}.  The problem was that the
anti-lock braking system would experience a ``short delay'' when
the brakes where pressed by the driver of the vehicle
\cite{thedetroitbureau.com:2009}.  This actually caused some crashes.
Toyota found that this short delay was the result of a software bug,
and was able to repair the the vehicles using a software update
\cite{Reuters:2009}.  Another incident where substantial harm was
caused was in 2002 where two planes collided over \"{U}berlingen in
Germany. A cargo plane operated by DHL collided with a passenger
flight holding fifty-one passengers.  Air-traffic control did not
notice the intersecting traffic until less than a minute before the
collision occurred.  Furthermore, the on-board collision detection
system did not alert the pilots until seconds before the collision.
It was officially ruled by the German Federal Bureau of Aircraft
Accidents Investigation that the on-board collision detection was
indeed faulty \cite{Collision:2004}.

The second major problem affects all of science.  Scientific
publications are riddled with errors.  A portion of these errors are
mathematical.  In 2012 Casey Klein et al. used specialized computer
software to verify the correctness of nine papers published in the
proceedings of the International Conference on Functional Programming
(ICFP).  Two of the papers where used as a control which where known
to have been formally verified before.  In their paper
\cite{Klein:2012} they show that all nine papers contained
mathematical errors.  This is disconcerting especially since most
researchers trust published work and base their own work off of these
papers.  Kline's work shows that trusting published work might result
in wasted time for the researchers basing their work off of these
error prone publications.  Faulty research hinders scientific
progress.

Both problems outlined above have been the focus of a large body of
research over the course of the last forty years.  These challenges
have yet to be completed successfully.  The work we present here makes
up the foundations of one side of the programs leading the initiative
to build theory and tools which can be used to verify the correctness
of software and mathematics.  This program is called program
verification using dependent type theories.  The second program is
automated theorem proving.  In this program researchers build tools
called model checkers and satisfiability modulo-theories solvers.
These tools can be used to model and prove properties of large complex
systems carrying out proofs of the satisfiability of certain
constraints on the system nearly automatically, and in some cases
fully automatically.  As an example Andr\'{e} Platzer and Edmund
Clarke in 2009 used automated theorem proving to verify the
correctness of the in flight collision detection systems used in
airplanes.  They actually found that there were cases where two planes
could collide, and gave a way to fix the problem resulting in a fully
verified algorithm for collision detection.  That is he mathematically
proved that there is no possible way for two planes to collide if the
systems are operational \cite{DBLP:conf/fm/PlatzerC09}.  Automated
theorem provers, however, are tools used to verify the correctness of
software externally to the programming language and compiler one uses
to write the software.  In contrast with verification using dependent
types we wish to include the ability to verify software within the
programming language being used to write the software. Both programs
have their merits and are very fruitful and interesting.

Every formal language within this thesis has been formally defined in
a tool called Ott \cite{Sewell:2010}.  In addition, the full Ott
specification of every type theory defined with in this thesis can be
found in the appendix.  Ott is a tool for writing definitions of
logics, programming languages, type theories, $\lambda$-calculi, and
any other formal language that consists of syntax and inference-style
rules.  Ott generates a parser and a type checker which is used to
check the accuracy of all objects definable with in the language given
to Ott as input.  Ott's strongest application is to check for syntax
errors within research articles.  Ott is a great example of a tool
using the very theory we are presenting in this thesis.  It clearly
stands as a successful step towards the solution of the second major
problem outlined above.
% section introduction (end)

\section{Novel Contributions}
\label{sec:novel_contributions}
This thesis consists of two major topics.  The first topic is on the
design of general purpose dependently-typed functional programming
languages. This topic is covered in Part~\ref{part:design} (Design).
The second topic is on the analysis of dependently-typed functional
programming languages and various type theories.  This topic is broken
up into two parts: Part~\ref{part:basic-analysis} (Basic Syntactic
Analysis) and Part~\ref{part:norm-hs} (Normalization by Hereditary
Substitution).  It is the content of these parts that consists of
novel research contributions. Specifically, the following list briefly
outlines each contribution:
\begin{itemize}
\item The design and analysis of a core dependently-typed functional
  programming language with a new property called freedom of
  speech. See Chapter~\ref{chap:freedom_of_speech} and
  Chapter~\ref{chap:freedom_of_speech_anal}.

\item The full design of a core dependently-typed functional
  programming language called Separation of Proof from Program
  ($\Sep$) that remedies several short comings of the freedom of
  speech language.  This is a full programming language that has been
  implemented, but this implementation is not a contribution of this
  thesis.  See Chapter~\ref{chap:separation_of_proof_from_program}.
  Part of $\Sep$'s design as well as several real-world examples of
  verification carried out in $\Sep$ were published in the special
  issue on advanced programming techniques for construction of robust,
  general and evolutionary programs \cite{Kimmel:2012}.

\item The design and analysis of a new logic and corresponding type
  theory called Dualized Intuitionistic Logic (DIL) and Dualized Type
  Theory (DTT) respectively. We introduce a new completely symmetric
  syntax that makes for a beautiful definition of the two theories.
  See Chapter~\ref{chap:dualized_type_theory_de} and
  Chapter~\ref{chap:dualized_type_theory_anal}.

\item We prove weak normalization of an entire family of predicative
  type theories based on Stratified System F using a proof technique
  called hereditary substitution. See
  Chapter~\ref{chap:stratified_system_f_and_beyond}.  A slightly
  different version of the proof of normalization using hereditary
  substitution of Stratified System F given in this thesis was
  presented at the workshop on proof-search in type theories
  \cite{Eades:2010}.

\item Similarly, we show that hereditary substitution can be extended
  to prove normalization of a classical type theory called the
  $\lambda\Delta$-calculus. See
  Chapter~\ref{chap:the_lambdadelta-calculus}.  This work first
  appeared at the workshop on control operators and their semantics
  \cite{Eades:2013}.

\item The final contribution is the brief history of type theory given
  in Part~\ref{part:background}.  There we try and highlight the
  significant contributions of type theory starting with Russell.
  This history is by no means complete, but we provide the complete
  definition of many significant type theories.  This tries to provide
  a one stop shop for an introduction to the field.
\end{itemize}
% section novel_contributions (end)

\part{Background}          
\label{part:background}
\import*{background/}{background}
% part background (end)

\part{Design}
\label{part:design}
The design of any programming language must facilitate reasoning about
the programming language itself.  This facilitation comes in the form
of a rigorous definition which makes mathematically precise all of the
structure of the programming language.  This provides a means for
researchers and implementors to fully understand the limits of the
language.  Contrary to what programming language designers have done
in the past this rigorous definition does not only include the syntax,
but also includes mathematical definitions of the type system as well
as an interpreter. This is a significant benefit of basing programming
languages on type theories.  As we have seen a type theory must be
rigorously defined.  

Considering the motivation we gave in the introduction, a
mathematically rigorous design provides the necessary structure to
allow programmers to mathematically reason about the programs they
write in the programming language.  In addition, this makes it
possible for designers to reason about the correctness of the language
itself, and thus allow them to make strong guarantees of the
correctness of the language to their programmers.  To prevent major
bugs programming languages must be rigorously defined.  If we do not
know what the programming language allows, then we cannot be sure what
is safe.

Recall from Chapter~\ref{chap:the_three_perspectives} that the
computational trinity\index{Computational Trinity} states that type
theories are simultaneously a logic as well as a programming language,
and this double perspective provides the means of verifying properties
of the programs we write in the theory, but this feature comes with a
strong invariant, every program must terminate.  Now suppose we are
working on a large development in a programming language based on type
theory like Martin-L\"of's Type Theory\index{Martin-L\"of's Type
  Theory (MLTT)} or CoC\index{Calculus of Constructions (CoC)} which has come to a
difficult problem that can be solved by the definition of some
terminating function.  Furthermore, suppose we have found the solution
in some research paper, but the termination proof is very complex and
uses an advanced semantic proof technique, and lastly suppose that
this termination proof is not formalized in a type theory, but is an
informal proof that we trust.  Since we are conducting our development
in a terminating type theory there is only one course of action.  We
have to formalize this complex termination argument. This can be
devastating to the development, because doing such a proof may take
months, even years to complete!  Another hypothetical is supposing we
do not care if a program terminates.  There are correctness properties
that hold regardless of termination.  For example, consider
associativity of list append -- using a pseudo syntax:
\[ \forall (A\,B\,C :
\mathsf{List}\,\mathsf{Nat}).\mathsf{append}\,A\,(\mathsf{append}\,B\,C)
= \mathsf{append}\,(\mathsf{append}\,A\,B)\,C \] This equation holds
regardless of termination of $A$, $B$, or $C$, because if anyone of
them happen to diverge, then both sides of the equation diverge.  

So the natural question is can we design a programming language based
on type theory that allows for non-terminating programs, but also be
able to verify properties of the programs we write in the language?
It turns out that we can, and in the next two chapters we introduce
the designs of two new dependently-typed functional programming
languages called Freedom of Speech and Separation of Proof from
Program that amount to solutions to this question.

Directly supporting non-termination is one solution, but there is
another possibility.  Non-terminating functions can be simulated using
the notion of coinduction which is dual to induction.  Coinduction
provides a means to define and observe infinite streams of data.
Furthermore, coinductive types push the notion of non-termination into
the types. There are a large number of applications of coinduction,
and even more when we allow the mixture of induction and coinduction,\index{Induction}\index{Coinduction}
for example the definition of an infinite stream of trees requires
their mixture.  However, currently there is no known type theory that
supports both induction and coinduction, and supports their
(unrestricted) mixture while maintaining type safety\index{Type Safety}.  In the third
chapter of Design we introduce a new logic and corresponding type
theory called Dualized Intuitionistic Logic and Dualized Type Theory
respectively that is based on a logic rich in duality that shows
promise of being a logical framework for induction and coinduction
that is type safe and supports their mixture.

\chapter{Freedom of Speech}
\label{chap:freedom_of_speech}
\import*{free-speech/}{FS-design-inc}
% chapter freedom_of_speech (end)

\chapter{Separation of Proof from Program}
\label{chap:separation_of_proof_from_program}
\import*{sep3/}{sep3-inc}
% chapter separation_of_proof_from_program (end)

\chapter{Dualized Logic and Type Theory}
\label{chap:dualized_type_theory_de}
\import*{dil-dtt/}{design-inc}
% chapter dualized_type_theory (end)
% part design (end)

\part{Basic Syntactic Analysis}
\label{part:basic-analysis}
The following chapters conduct basic meta-theoretic analysis of the
freedom of speech language, and dualized logic and its corresponding
type theory dualized type theory.  We consider these analysis to be
basic because they are straightforward applications of existing
techniques, and consist of the most basic properties that should hold
for any programming language.

At least a basic meta-theoretic analysis of a programming language is
very important.  Every programming language should at least have been
proven type safe\index{Type Safety}, and if the language contains a
logical fragment, then it should be proven consistent.  These give
strong guarantees to the programmer.  Type safety ensures that if a
program has a type, then it can either compute something, or is the
answer itself.  Furthermore, it ensures that if a program has a type,
then after running it, the result has the same type.  Lastly,
consistency\index{Consistency} -- as we have mentioned before --
ensures that the objects of the language we call proofs really are
proof.  These sound like commonsense properties, and they are, but
many programming language implementors fail to establish them.  A
programming language with such guarantees higers one confidence in the
correctness and consistency of the language.  Therefore, a basic
meta-theoretic analysis is necessary to be able to trust the programs
we write, and the verification of programs carried out within the
languages themselves.

\chapter{Freedom of Speech}
\label{chap:freedom_of_speech_anal}
\import*{free-speech/}{FS-analysis-inc}
% chapter freedom_of_speech (end)

\chapter{Dualized Logic and Type Theory}
\label{chap:dualized_type_theory_anal}

In Section~\ref{chap:dualized_type_theory_de} we introduced Dualized
Intuitionistic Logic (DIL) and its corresponding type theory Dualized
Type Theory (DTT).  Now we present the basic metatheory of both DIL
and DTT.  We start with proving consistency of DIL, and then prove
completeness by reduction to Pinto and Uustalu's L. Then we move onto
show type preservation and strong normalization for DTT.  We show the
latter using a version of Krivine's classical realizability by
translating DIL into a classical logic.

\import*{dil-dtt/}{analysis-inc}
% chapter dtt (end)
% part basic-analysis (end)

\part{Normalization by Hereditary Substitution}
\label{part:norm-hs}

One motivation for the work in this thesis is that programming
languages must contain the means of verifying the software written in
them.  This verification would catch major bugs during development as
opposed to after the software is released to the public.  For example,
the bug found in the breaking system of the 2010 Prius would have
never been released, and thus no accidents would have occurred.  A
programming language that contains the ability to verify programs
would have to contain some notion of a logic, and this logic must be
trusted.  That is, the proofs written in this logic must be true; this
corresponds to logical consistency\index{Consistency}.

In Section~\ref{sec:hereditary_substitution} we introduce the
hereditay substitution proof technique for showing normalization and
metioned that has one caveat, it is not known which type theories it
can be applied to -- see Section~\ref{sec:hereditary_substitution} for
a list of type theories that are known to be proven normalizing using
hereditary substitution, and a list of some that are not.  The
contribution of each of the following chapters is to widen the set of
type theories hereditay substitution can be applied to.  We first give
several proofs of normalization using hereditary substitution for
several extensions of stratified system F (SSF), and then give the
first proof of normalization by hereditary substitution for a
classical type theory called the $\lambda\Delta$-calculus.  Throughout
all of the following chapters we assume the reader is familiar with
hereditary substitution.  If the reader is not, then they should first
read Section~\ref{sec:hereditary_substitution}.

\chapter{Stratified System F and Beyond}
\label{chap:stratified_system_f_and_beyond}
\import*{norm-by-hs/ssf/}{ssf.tex}
\import*{norm-by-hs/ssf+/}{ssf+.tex}
\import*{norm-by-hs/dep-ssf/}{dep-ssf.tex}
% \import*{norm-by-hs/ssf-omega/}{ssf-omega.tex}
% chapter stratified_system_f_and_beyond (end)

\chapter{The $\lambda\Delta$-Calculus}
\label{chap:the_lambdadelta-calculus}
\import*{norm-by-hs/lam-delta/}{lam-delta-inc.tex}
% chapter the_lambdadelta-calculus (end)

% part norm-hs (end)

\chapter{Conclusion}
\label{chap:conclusion}

Bugs are the bane of software development, and in order to rid the
world of such problems we must revisit the very foundations of
programming languages.  We have argued that a programming language
must be mathematically defined.  This allows programmers and language
designers the ability to reason about the software being developed in
the language.  Furthermore, we have argued that the programming
languages of the future must contain some notion of a logic which
allows for the verification of the programs written in the languages.

We introduced two new dependently-typed functional programming
languages that contain a logical fragment called Freedom of Speech and
Separation of Proof from Program.  We proved logical consistency of
the logical fragment of the former.  Furthermore, we introduced a new
type theory called Dualized Type Theory that shows promise of being a
logical foundation of induction and co-induction.  All of these new
theories constitute foundations of programming languages that directly
support verification of the software developed in them.

Finally, we showed how to adapt the hereditary substitution proof
technique for showing logical consistency by establishing
normalization of several extensions of Stratified System F.  We also
proved normalization by hereditary substitution of the
$\lambda\Delta$-calculus; the first classical type theory to be proved
consistent using hereditary substitution.  These proofs show that
hereditary substitution is a valuable tool that can be used to
establish consistency of the logical fragment of various programming
languages.

We believe the future is bright, and that investigations such as the
ones in this thesis will lead to new and exciting programming
languages.  These new languages will have the means to prevent major
bugs in safety critical devices from reaching the public, but there is
a lot more work to be done.

\section{Future Work}
\label{sec:future_work}
The following list addresses future work related to each theory:
\begin{itemize}
\item[-] The freedom of speech language has a call-by-value
  operational semantics, but this requires many unfortunate value
  restrictions.  It would be worthwhile to explore the language design
  when call-by-name is adopted.  Some questions to ask are is the
  language more elegant?  That is, do we trade the value restrictions
  for some other restrictions?

\item[-] We only presented the design of the $\Sep$, but the analysis
  still needs to be done.  In particular, consistency should be
  established for the proof fragment, and type safety should be proven
  for the entire language.

\item[-] DTT is a nice start at understanding if bi-intuitionistic
  logic can be the basis of a logical framework for induction and
  co-induction, but it is not clear that DTT can be conservatively
  extended with such features and maintain type safety and
  consistency.  An exploration of these features is an important
  future goal.  Furthermore, the language design containing
  abstractions from the semantics (abstract Kripke graphs, and worlds
  on types) is an unfortunate consequence of the failure of
  cut-elimination due to subtraction, are there alternate designs that
  could be used which prevent the need for these abstractions?

\item[-] Hereditary substitution still needs to be explored with
  respect to more expressive type theories.  For example, can it be
  used to prove normalization for a type theory with large
  eliminations?  How about impredicative theories like system F?  My
  conjecture is that it can, but it will require new ideas, because
  the orderings we have used to prove correctness of the hereditary
  substitution function are too weak.
\end{itemize}
% section future_work (end)
% section conclusion (end)


%=============================================================================
\appendix
%=============================================================================

\chapter{Type Theories}
\label{chap:type_theories}
\section{The $\lambda$-Calculus}
\label{sec:the_lambda-calculus}
\Lamall{}
% section the_lambda-calculus (end)

\newpage
\section{Church-Style Simply Typed $\lambda$-Calculus}
\label{sec:church_style_simply_typed_lambda-calculus}
\CHSTLCall{}
% section church_style_simply_typed_lambda-calculus (end)

\newpage
\section{Curry-Style Simply Typed $\lambda$-Calculus}
\label{sec:curry_style_simply_typed_lambda-calculus}
\CSTLCall{}
% section curry_style_simply_typed_lambda-calculus (end)

\newpage
\section{Combinatory Logic}
\label{sec:combinatory_logic}
\Comball{}
% section combinatory_logic (end)

\newpage
\section{G\"odel's System T}
\label{sec:godels_system_t}
\Tall{}
% section godels_system_t (end)

\newpage
\section{Girard-Reynold's System F}
\label{sec:girard-reynolds_system_f}
\Fall{}
% section girard-reynolds_system_f (end)

\newpage
\section{Stratified System F}
\label{sec:stratified_system_f}
\SSFall{}
% section girard-reynolds_system_f (end)

% \newpage
% \section{System $\text{F}^\omega$}
% \label{sec:system_fw}
% \Fwall{}
% % section system_fw (end)

\newpage
\section{The $\lambda\mu$-Calculus}
\label{sec:lamu_all}
\Lamuall{}
% section lamu_all (end)

\newpage
\section{The $\lambda\Delta$-Calculus}
\label{sec:lamd_all}
\Lamdall{}
% section lamd_all (end)

\newpage
\section{The $\LBMMT$-Calculus}
\label{sec:lbmmt_all}
\LBMMTall{}
% section lbmmt_all (end)

\newpage
\section{The Dual-Calculus}
\label{sec:dc_all}
\DCall{}
% section dual_calculus_all (end)

\newpage
\section{Martin-L\"of's Type Theory}
\label{sec:tt_all}
\TTall{}
% section tt_all (end)

\newpage
\section{The Calculus of Constructions}
\label{sec:coc_all}
\CoCall{}
% section coc_all (end)

\newpage
\section{The Separated Calculus of Constructions}
\label{sec:coc_sep_all}
\CoCSall{}
% section coc_sep_all (end)

\newpage
\section{Freedom of Speech}
\label{sec:freedom_of_speech_all}
\FSall{}
% section freedom_of_speech_all (end)

\newpage
\section{Annotated Separation of Proof from Program}
\label{sec:annotated_separation_of_proof_from_program}
\Sepall{}
% section annotated_separated_of_proof_from_program (end)

\newpage
\section{Unannotated Separation of Proof from Program}
\label{sec:unannotated_separation_of_proof_from_program}
\SepUall{}

\import*{sep3/}{sep3-app-inc}
% section annotated_separation_of_proof_from_program (end)

\newpage
\section{Pinto and Uustalu's L}
\label{sec:pinto_and_uustalu's_l}
\Lall{}
% section pinto_and_uustalu's_l (end)

\newpage
\section{Dualized Intuitionistic Logic, Dualized Type Theory, and
  Dualized Classical Logic}
\label{sec:dualized_intuitionistic_logic}
\dttall{}
% section dualized_intuitionistic_logic (end)
% section type_theories (end)

%=============================================================================
% bibliography
%=============================================================================
\interlinepenalty=10000	% prevents bib items from splitting across pages
\bibliographystyle{uithesis}
\bibliography{thesis}

\clearpage
\addcontentsline{toc}{chapter}{Index}
\IfFileExists{thesis.ind}{\input{thesis.ind}}{}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
