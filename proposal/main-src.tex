\usepackage{amsmath,amssymb,amsthm,amstext}
\usepackage{mathpartir}
\usepackage{vmargin}
\usepackage{enumitem}
\usepackage{url}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{stmaryrd} 
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{hyperref}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}

\newcommand{\redto}[0]{\rightsquigarrow}
\newcommand{\interp}[1]{\llbracket #1 \rrbracket}
\newcommand{\ifrName}[1]{#1}
\newcommand{\Sep}[0]{\text{SepPP}}

\newcommand{\chcalc}[0]{\bar{\lambda}\mu\tilde{\mu}}

\newcommand{\cse}[0]{\noindent\underline{\textbf{Case:}}\ }


\newcommand{\ndto}[1]{\to_{#1}}
\newcommand{\ndwedge}[1]{\wedge_{#1}}

\newcommand{\To}[0]{\Rightarrow}

\begin{document}

\title{Semantic Analysis of Advanced Programming Languages}

\author{Harley Eades III\\ Computer Science\\ The University of Iowa}

\date{}

\maketitle

\section{Thesis: Introduction}
\label{sec:introduction}
There are two major problems growing in two areas.  The first is in
Computer Science, in particular software engineering. Software is
becoming more and more complex, and hence more susceptible to software
defects.  Software bugs have two critical repercussions: they cost
companies lots of money and time to fix, and they have the potential
to cause harm. 

The National Institute of Standards and Technology estimated that
software errors cost the United State's economy approximately sixty
billion dollars annually, while the Federal Bureau of Investigations
estimated in a 2005 report that software bugs cost U.S. companies
approximately sixty-seven billion a year \cite{nist02,fbi05}.

Software bugs have the potential to cause harm.  In 2010 there were a
approximately a hundred reports made to the National Highway Traffic
Safety Administration of potential problems with the braking system of
the 2010 Toyota Prius \cite{Consumer:2010}.  The problem was that the
anti-lock braking system would experience a ``short delay'' when
the brakes where pressed by the driver of the vehicle
\cite{thedetroitbureau.com:2009}.  This actually caused some crashes.
Toyota found that this short delay was the result of a software bug,
and was able to repair the the vehicles using a software update
\cite{Reuters:2009}.  Another incident where substantial harm was
caused was in 2002 where two planes collided over \"{U}berlingen in
Germany. A cargo plane operated by DHL collided with a passenger
flight holding fifty-one passengers.  Air-traffic control did not
notice the intersecting traffic until less than a minute before the
collision occurred.  Furthermore, the on-board collision detection
system did not alert the pilots until seconds before the collision.
It was officially ruled by the German Federal Bureau of Aircraft
Accidents Investigation that the on-board collision detection was
indeed faulty \cite{Collision:2004}.

The second major problem affects all of science.  Scientific
publications are riddled with errors.  A portion of these errors are
mathematical.  In 2012 Casey Klein et al. used specialized computer
software to verify the correctness of nine papers published in the
proceedings of the International Conference on Functional Programming
(ICFP).  Two of the papers where used as a control which where known
to have been formally verified before.  In their paper
\cite{Klein:2012} they show that all nine papers contained
mathematical errors.  This is disconcerting especially since most
researchers trust published work and base their own work off of these
papers.  Kline's work shows that trusting published work might result
in wasted time for the researchers basing their work off of these
error prone publications.  Faulty research hinders scientific
progress.

Both problems outlined above have been the focus of a large body of
research over the course of the last forty years.  These challenges
have yet to be completed successfully.  The work I present here makes
up the foundations of one side of the programs leading the initiative
to build theory and tools which can be used to verify the correctness
of software and mathematics.  This program is called program
verification using dependent type theories.  The second program is
automated theorem proving.  In this program researchers build tools
called model checkers and satisfiability modulo-theories solvers.
These tools can be used to model and prove properties of large complex
systems carrying out proofs of the satisfiability of certain
constraints on the system nearly automatically, and in some cases
fully automatically.  As an example Andr\'{e} Platzer and Edmund
Clarke in 2009 used automated theorem proving to verify the
correctness of the in flight collision detection systems used in
airplanes.  They actually found that there were cases where two plans
could collide, and gave a way to fix the problem resulting in a fully
verified algorithm for collision detection.  That is he mathematically
proved that there is no possible way for two plans to collide if the
systems are operational \cite{DBLP:conf/fm/PlatzerC09}.  Automated
theorem provers, however, are tools used to verify the correctness of
software externally to the programming language and compiler one uses
to write the software.  In contrast with verification using dependent
types we wish to include the ability to verify software within the
programming language being used to write the software. Both programs
have their merits and are very fruitful and interesting.

This report summarizes my dissertation by part and chapter. Each
section will be given the name of a part, and then the contents of the
section will consist of a summery of that part.  Similarly,
subsections will summarize chapters.  I make sure to include my
already published work as well as on going work that needs to be done
before my defense.  My thesis will be broken into five main parts.
The first, gives a history of type theory, and the necessary
background to facilitate understanding of the main results. The second
part is on the design of new advanced functional programming
languages.  The third covers the basic syntactic-analysis of various
type theories and functional programming languages.  The forth covers
normalization by hereditary substitution.  Finally, the fifth covers
categorical semantics of type theories.
% section introduction (end)

\section{Part 0: History and Background}
\label{sec:history_and_background}
This part provides a brief history of type theory as a foundation of
mathematics and typed-functional programming languages.  It begins
with Bertrand Russell -- the founder of type theory -- and introduces
key results up to the present. This part also serves as an
introduction of all the necessary concepts to understand the remainder
of the thesis.  I make sure and present each type theory in its
entirety and rigorously.  In fact every language defined in the thesis
will be formally defined in Ott \cite{Sewell:2010}.  Ott is a tool for
writing definitions of programming languages, type theories, and
$\lambda$-calculi.  Ott generates a parser and a type checker which is
used to check the accuracy of all objects definable within the
language given to Ott as input.  Ott's strongest application is to
check for syntax errors within research articles.  It is a great
example of a tool using the very theory I will present in my thesis.
It clearly stands as a successful step towards the solution of the
second major problem outlined in the introduction.  Lastly, this
history and background has all been written and was presented as my
comprehensive exam.
% section history_and_background (end)

\section{Part I: Design}
\label{sec:design}

This part presents the design of a two general-purpose
dependently-typed functional programming languages called Freedom of
Speech, and Separation of Proof from Program, and a new constructive
type theory with constructive control called Dualized Type Theory.
This part will have a chapter per language. All the work with
respect to the first two languages is complete although mostly
unpublished.  The language of Dualized Type Theory is stabilizing, but
its analysis is on going work \cite{Stump:2013}.

The TRELLYS project is a collaboration between the University of Iowa,
University of Pennsylvania, and Portland State University to design a
new general-purpose dependently-typed functional programming language
that supports type-based verification.  What sets TRELLYS apart from
other similar projects is that it contains a number of advanced
features within the same language.  For example, mixing type-based
verification with general recursion is not well understood.  This
mixture is one of the primary aims of this project.  My primary
contribution to this project was to the design and analysis of the
core.  I helped with the design and analysis of two core languages.
The first is Freedom of Speech.

\subsection{Chapter I: Freedom of Speech}
\label{subsec:freedom_of_speech}
The main objective of the TRELLYS project is to design a functional
programming language with two discernible fragments: a logical
fragment, and a programmatic fragment.  The programmatic fragment is a
general purpose dependently-type functional programming language.
This is a Turing-complete language with general recursion.  In
addition it contains the type in type axiom which leads to paradoxes
\cite{Coquand:1986,Coquand:1994}.  This axiom allows for the
definition of generic programs.  Now the logical fragment is the
fragment that can be used to prove properties about the programs
defined in the programmatic fragment.  In order for this fragment to
be considered a logic it must terminating.  Meaning every program
written in the this fragment must terminate in general.  This property
guarantees that the logic is consistent.

Recall that the three perspectives of computation\footnote{Also known
  as the Curry-Howard correspondence, and the propositions-as-types
  proofs-as-programs correspondence.} tell us that programs are proofs
and their types are propositions.  The most significant feature of
Freedom of Speech is that logical types can contain programs from the
programmatic fragment, but they are never allowed to be applied to any
arguments.  Thus, the logical fragment is allowed to ``talk'' about
the programmatic fragment.  This property we call free speech.

This chapter introduces the Freedom of Speech language and discusses
its design.  The analysis of this language greatly influenced its
design.  I discuss the analysis in
Section~\ref{subsec:free_speech}.  Interesting properties of this
design include: implicit arguments, the free speech property, a
collapsed syntax, and judgmental fragmentation into the logical and
programmatic fragment.  This design is complete and formalized in Ott,
but was never published.
% subsection freedom_of_speech (end)

\subsection{Chapter II: Separation of Proof from Program ($\Sep$) }
\label{subsec:separation_of_proof_from_program}
One of the key features of the Freedom of Speech language is that it
has a collapsed language where terms and types are apart of the same
syntactic category.  This makes for a very elegant design, but due to
having to fragment the language into a logical fragment and a
programmatic fragment using the typing judgment the language became
very hard to reason about.  This is especially true when we want to
extend the Freedom of Speech language with additional features.  For
example, algebraic data types.  To overcome this problem we (the
University of Iowa group) proposed to separate the logical fragment
from the programmatic fragment into two distinct languages. Then
provide the necessary linkage to allow for the logical fragment to
contain the free-speech property.

In this section the full design of $\Sep$ is detailed and discussed.
We did not carry out any analysis of this language, but only explored
its design, and its use in writing real-world programs
\cite{Kimmel:2013,Kimmel:2012}.  The full design of $\Sep$ has never
been published, however, its design is completely written up, and
an implementation\footnote{$\Sep$'s implementation was authored by
  Dr. Garrin Kimmel.}  exists.
% subsection separation_of_proof_from_program (end)

\subsection{Chapter III: Dualized Type Theory (DTT)}
\label{subsec:dualized_type_theory}
The Freedom of Speech language did not contain algebraic data types.
$\Sep$ added data types, but separated the two worlds all together.
In addition, neither language contained coinductive data types.  These
are data types representing infinite objects.  For example, streams.
So how could we add coinductive data types?  To make matters a bit
more interesting, how could we add coinductive data types in such a
way that inductive-coinductive data types could be defined?  That is,
can we allow for the mixture of inductive and coinductive data?  This
is not a well understood feature.  In fact, a general framework for
inductive and coinductive data is an open problem.

A candidate for a general framework for inductive and coinductive data
types begins with a logic called Bi-Intuitionistic Logic (BINT). This
is a constructive logic where for every logical connective its dual
connective is a logical connective of the logic.  For example, BINT
contains disjunction and conjunction, and implication and its dual, a
connective called subtraction or exclusion.  In this chapter I detail
the logic of BINT called Dualized Logic (DIL), and a term assignment
to DIL called Dualized Type Theory (DTT).  I will also discuss its
potential as a general framework for inductive and coinductive data.
I also think DTT provides a nice setting for the study of constructive
control operators. Constructive control operators are control
operators -- operators that have the ability to discard evaluation
contexts -- that have been restricted to discarding only certain
context so as to remain constructive.  The design of DIL and DTT is
stabilizing, but its analysis is on going work \cite{Stump:2013}.
% subsection dualized_type_theory_(dtt) (end)
% section design (end)

\section{Part II: Basic-Syntactic Analysis}
\label{sec:syntactic_analysis}

There is a cardinal rule one must follow when designing programming
languages.  This rules states that one must carry out at least a
basic-syntactic analysis of the programming language one is designing.
A basic-syntactic analysis includes proving that the programming
language is type safe.  If the language contains a logical fragment
then this fragment must be proven consistent.  This part presents this
analysis for the Freedom Speech language and DTT.

\subsection{Chapter I: Freedom Speech}
\label{subsec:free_speech}
This chapter presents a preservation proof of the Freedom Speech
language.  Then using this proof I prove weak normalization for the
logical fragment.  These results do not use any new mathematical
machinery, but adapts well-known techniques to this language.  There
are however a few non-trivial problems which had to be overcome, which
this chapter will take care in explaining.  This analysis is complete
and ready to be included in my thesis, however it is unpublished.
% subsection free_speech (end)

\subsection{Chapter II: Dualized Type Theory}
\label{subsec:dtt}
The analysis of DTT begins with the analysis of DIL.  We prove that
DIL is consistent, and complete with respect to a semantics for BINT
using Kripke models.  The consistency proof is complete and has been
formalized in the interactive proof assistant Agda \cite{Norell:2007}.
I am currently finishing up the completeness proof, but on paper.

Now that I know I have a sound and complete bi-intuitionistic logic
I add a term assignment to DIL.  This takes the logic and produces a
type theory where proofs are programs and propositions are types.  In
addition to this term assignment we must define a reduction relation
which gives the necessary rules for computaiton.  That is, the
reduction relation relates programs by taking a single step of
computaiton. Then we must prove this reduction relation is type safe,
and that it is terminating, because DTT is a logic.  We want to use it
for logical reasoning, and thus it cannot contain infinite proofs.  I
conjecture these properties hold, but they still need to be carried
out.  This analysis should be strightforward using known techniques.
I plan to have this work completed by Febuary 2014 for publication.
% subsection the_analysis_of_dtt (end)
% section syntactic_analysis (end)

\section{Part III: Normalization by Hereditary Substitution}
\label{sec:normalization_by_hereditary_substitution}
One of the most difficult meta-theoretic results is proving
normalization whether strong or weak.  The latter is often easier, and
is enough to conclude consistency.  Therefore, it is a worthy task to
try and find techniques that make these proofs less difficult.  A
technique which shows promise of accomplishing this feat is
normalization by hereditary substitution.  Hereditary substitution is
a metalevel function which substitutes a term into another and
recursively reduces any redexes that are created as a result of the
substitution.  Then using this function we can prove weak
normalization of the type theory.  Identifying the set of type
theories that can be proven weakly normalizing is an open problem.
This part identifies several type theories in this set that were
previously unknown to be amenable to this style of analysis.

\subsection{Chapter I: Stratified System F (SSF) and its Extensions}
\label{subsec:stratified_system_f_ext}
SSF is a predicative polymorphic type theory.  It can be seen as a
restriction of Girard's system F \cite{Leivant:1991}. In this chapter
we prove weak normalization using hereditary substitution for SSF and
several of its extensions. The extensions are as follows:
\begin{itemize}
\item[-] SSF,
\item[-] SSF extended with sum types and commuting conversions,
\item[-] SSF extended with equality types, and
\item[-] SSF extended with type-level computation.
\end{itemize}
\noindent
All of these results are finished \cite{Eades:2011,Eades:2010}.
% subsection stratified_system_f_ext (end)

\subsection{Chapter II: The $\lambda\Delta$-Calculus}
\label{subsec:the_lambdadelta-calculus}
The hereditary substitution proof technique was only applied to
constructive type theories, but what about classical type theories? My
advisor and I were the first to apply this technique to type theories
with control operators.  We choose the $\lambda\Delta$-calculus due to
it being a straightforward extension of the simply-typed
$\lambda$-calculus.  The $\lambda\Delta$-calculus adds the law of
double negation to the simply-typed $\lambda$-calculus, thus, the law
of excluded middle is provable.  This extension adds a new operator
called the $\Delta$-operator which can be used to define familiar
control operators like $call/cc$.  This chapter presents this result
by first defining the $\lambda\Delta$-calculus and then proving it
weakly normalizing using hereditary substitution.  This result has
been published in \cite{Eades:2013}.
% subsection the_lambdadelta-calculus (end)

% section normalization_by_hereditary_substitution (end)

\section{Part IV: Categorical Semantic }
\label{sec:categorical_semantics}

Category theory is a field of abstract mathematics.  It is the
abstract study of functions and function composition.  One might find
the previous statement interesting, because one can consider the
$\lambda$-calculus as a theory of functions and function composition.
J. Lambek showed that the type $\lambda$-calculus is in one-to-one
correspondence with cartesian-closed categories.  Since this wonderful
discovery category theory has been used to study programming
languages, and especially type theories.  Providing a categorical
model of a programming languages or type theory gives a unique
perspective of the theory.  The difficulty is figuring out what
categorical constructs one needs in order to model the language.  A
categorical model for bi-intuitionistic logic is a current open
problem.  In this part I provide three novel categorical models of
bi-intuitionistic logic.  All the work in this part is on going.

\subsection{Chapter I: Split Bi-Intuitionistic Logic}
\label{subsec:split_bi-intuitionistic_logic}

This formalization of bi-intuitionism came out of studying various
categorical models.  The idea is in the same spirit as $\Sep$. Split
BINT splits the language into two distinct syntactic categories: the
positive fragment, and the negative fragment. The positive fragment
has a categorical model in cartesian-closed categories, while the
negative fragment has a categorical model in the opposite category.
So conjunction, implication, and absurdity are the logical constructs of
the positive fragment, while disjunction, subtraction, and truth, are
the logical constructs of the negative fragment.  The two fragments are
then linked together by two functors representing two negations.
These allow for the mixture of both the fragments and is the source of
the bi-intuitionism.  In terms of Kripke models the logical
connectives of the positive fragment consider all future worlds, while
the negative fragment considers the existence of past worlds.  The
negations change the quantifier from a universal to an existential and
vice versa.  The design and analysis of this formalization is on
going, but the categorical model has been formalized in Agda.
% subsubsection split_bi-intuitionistic_logic (end)

\subsection{Chapter II: Dualized Type Theory}
\label{subsubsec:dualized_type_theory}
One key property of constructive logic that has been hard to obtain in
bi-intuitionistic logic is cut elimination.  Regaining this key
property is the reason why DIL and DTT have a very unique sequent.
The usual sequent of propositional intuitionistic logic is something
like $\Gamma \vdash \Delta$ where $\Gamma$ is a multiset of hypotheses
and $\Delta$ is a multiset of conclusions.  To express the idea that
subtraction considers pasts worlds we had to pull the structure of the
Kripke model into the object language of the logic and type theory.
Thus, our sequent changes to $G ; \Gamma \vdash \Delta$ where each
hypothesis and conclusion is associated with a node of the graph $G$.
This graph is an abstract Kripke model.  To give a categorical model
for DTT the category must capture this notion of an abstract Kripke
model.  For this I defined a novel category called preordered
categories.  This category has morphisms that have a domain, codomain,
and a preorder, where the domain and codomain are associated with an
object of the preorder.  I have begun proving soundness with respect
to preordered categories, and plan to have completeness proven, as
well as a proof that the equality in the type theory (DTT) and the
equality of the model are the same for my thesis.
% subsection dualized_type_theory (end)

\subsection{Chapter III: Nested Bi-Intuitionistic Logic}
\label{subsec:nested_bi-intuitionistic_logic}
Another method of regaining cut elimination was proposed by Gor\'e et
al. using a sequent that contains a subsequent within
it \cite{Gore:2010}.  Time permitting I plan to give a categorical
model of this style of logic using a higher-order category called
2-categories.  These have objects, morphisms, and, additionally,
morphisms between morphisms.  The latter would allow for the
interpretation of subsequents. If this works out then a very elegant
model in higher categories would exist and could potentially yield the
necessary structure of a more elegant logic.
% subsubsection nested_bi-intuitionistic_logic (end)
% section categorical_semantics_ (end)

\section{Conclusion}
\label{sec:conclusion}
This report serves as an outline of my dissertation.  It will consist
of five major parts on the design and analysis of advanced-programming
languages, type theories, and logic.  I have explicitly stated which
work is completed and which still needs to be done.  As a reminder the
following work still needs to be done:
\begin{itemize}
\item[-] The analysis of DIL and DTT,
\item[-] the design and categorical model of split BINT,
\item[-] the study of preordered categories and their use as a model of DTT, and
\item[-] the categorical model of nested BINT.
\end{itemize}
I plan to have all this work completed by April 2014, and defend my
dissertation in June 2014.
% section conclusion (end)

\bibliographystyle{plain} \bibliography{biblio}

